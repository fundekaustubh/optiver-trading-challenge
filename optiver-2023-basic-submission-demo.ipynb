{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import optiver2023\nenv = optiver2023.make_env()\niter_test = env.iter_test()","metadata":{"execution":{"iopub.status.busy":"2023-10-18T06:24:36.295580Z","iopub.execute_input":"2023-10-18T06:24:36.295932Z","iopub.status.idle":"2023-10-18T06:24:36.669983Z","shell.execute_reply.started":"2023-10-18T06:24:36.295906Z","shell.execute_reply":"2023-10-18T06:24:36.668942Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"counter = 0\nfor (test, revealed_targets, sample_prediction) in iter_test:\n    sample_prediction['target'] = 0\n    env.predict(sample_prediction)\n    counter += 1","metadata":{"execution":{"iopub.status.busy":"2023-10-18T06:24:41.569193Z","iopub.execute_input":"2023-10-18T06:24:41.569779Z","iopub.status.idle":"2023-10-18T06:24:42.286818Z","shell.execute_reply.started":"2023-10-18T06:24:41.569743Z","shell.execute_reply":"2023-10-18T06:24:42.285656Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"This version of the API is not optimized and should not be used to estimate the runtime of your code on the hidden test set.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### TOC\n1. Anomalies in Data, and cleaning action & explaination. 15 pts\n2. Pairwise Corralation Table and explaition. 10 pts\n3. Average records stockID vs Day, 25 pts\n    - a. autocorrelation, 10 pts\n    - b. measure the distance, 5 pts\n    - c. clustering algorithm, 10 pts\n4. Closing trajectory of stocks on each day highly correlated, 25 pts\n   - a. Make three plots, 10 pts\n   - b. permutation test to determine the statistical confidence, 15 pts\n      p-value\n5. Best prediction model, any approaches, 25 pts\n6. submit model on Kaggle, 0 pts\n\n#### Start\n- Copy this notebook.\n  In Google Colab use `File -> Save a Copy in Drive`.\n- Use the \"Text\" blocks to provide explanations wherever you find them necessary.\n- Highlight your answers inside these text fields to ensure that we don't miss it\nwhile grading your HW.\n\n#### Setup\n\n- Code to download the data directly from the colab notebook.\n- If you find it easier to download the data from the kaggle website (and\nuploading it to your drive), you can skip this section.","metadata":{}},{"cell_type":"code","source":"!ls '/kaggle/input/optiver-trading-at-the-close'","metadata":{"execution":{"iopub.status.busy":"2023-10-18T06:24:45.639618Z","iopub.execute_input":"2023-10-18T06:24:45.639984Z","iopub.status.idle":"2023-10-18T06:24:46.759006Z","shell.execute_reply.started":"2023-10-18T06:24:45.639943Z","shell.execute_reply":"2023-10-18T06:24:46.757593Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"example_test_files  optiver2023  public_timeseries_testing_util.py  train.csv\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### Q1: Anomalies and Cleaning, 15 pts","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ncol_names = [\n  \"stock_id\",\n  \"date_id\",\n  \"seconds_in_bucket\",\n  \"imbalance_size\",\n  \"imbalance_buy_sell_flag\",\n  \"reference_price\",\n  \"matched_size\",\n  \"far_price\",\n  \"near_price\",\n  \"bid_price\",\n  \"bid_size\",\n  \"ask_price\",\n  \"ask_size\",\n  \"wap\",\n  \"target\",\n  \"time_id\",\n  \"row_id\"\n]\ndtypes = {\n  \"stock_id\": np.int,\n  \"date_id\":np.int,\n  \"seconds_in_bucket\":np.int,\n  \"imbalance_size\":np.float64,\n  \"imbalance_buy_sell_flag\":np.int,\n  \"reference_price\":np.float64,\n  \"matched_size\":np.float64,\n  \"far_price\":np.float64,\n  \"near_price\":np.float64,\n  \"bid_price\":np.float64,\n  \"bid_size\":np.float64,\n  \"ask_price\":np.float64,\n  \"ask_size\":np.float64,\n  \"wap\":np.float64,\n  \"target\":np.float64,\n  \"time_id\":np.int,\n  \"row_id\": \"string\",\n}\ncsv = pd.read_csv(\"/kaggle/input/optiver-trading-at-the-close/train.csv\")","metadata":{"execution":{"iopub.status.busy":"2023-10-18T06:24:51.995502Z","iopub.execute_input":"2023-10-18T06:24:51.995950Z","iopub.status.idle":"2023-10-18T06:25:11.350903Z","shell.execute_reply.started":"2023-10-18T06:24:51.995911Z","shell.execute_reply":"2023-10-18T06:25:11.350023Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_72/1025626740.py:26: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\nDeprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n  \"stock_id\": np.int,\n/tmp/ipykernel_72/1025626740.py:27: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\nDeprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n  \"date_id\":np.int,\n/tmp/ipykernel_72/1025626740.py:28: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\nDeprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n  \"seconds_in_bucket\":np.int,\n/tmp/ipykernel_72/1025626740.py:30: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\nDeprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n  \"imbalance_buy_sell_flag\":np.int,\n/tmp/ipykernel_72/1025626740.py:41: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\nDeprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n  \"time_id\":np.int,\n","output_type":"stream"}]},{"cell_type":"code","source":"csv.head(1)","metadata":{"execution":{"iopub.status.busy":"2023-10-18T06:25:11.352769Z","iopub.execute_input":"2023-10-18T06:25:11.353069Z","iopub.status.idle":"2023-10-18T06:25:11.376252Z","shell.execute_reply.started":"2023-10-18T06:25:11.353035Z","shell.execute_reply":"2023-10-18T06:25:11.375180Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"   stock_id  date_id  seconds_in_bucket  imbalance_size  \\\n0         0        0                  0      3180602.69   \n\n   imbalance_buy_sell_flag  reference_price  matched_size  far_price  \\\n0                        1         0.999812   13380276.64        NaN   \n\n   near_price  bid_price  bid_size  ask_price  ask_size  wap    target  \\\n0         NaN   0.999812   60651.5   1.000026   8493.03  1.0 -3.029704   \n\n   time_id row_id  \n0        0  0_0_0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>stock_id</th>\n      <th>date_id</th>\n      <th>seconds_in_bucket</th>\n      <th>imbalance_size</th>\n      <th>imbalance_buy_sell_flag</th>\n      <th>reference_price</th>\n      <th>matched_size</th>\n      <th>far_price</th>\n      <th>near_price</th>\n      <th>bid_price</th>\n      <th>bid_size</th>\n      <th>ask_price</th>\n      <th>ask_size</th>\n      <th>wap</th>\n      <th>target</th>\n      <th>time_id</th>\n      <th>row_id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3180602.69</td>\n      <td>1</td>\n      <td>0.999812</td>\n      <td>13380276.64</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.999812</td>\n      <td>60651.5</td>\n      <td>1.000026</td>\n      <td>8493.03</td>\n      <td>1.0</td>\n      <td>-3.029704</td>\n      <td>0</td>\n      <td>0_0_0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"csv.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2023-10-18T06:25:51.245092Z","iopub.execute_input":"2023-10-18T06:25:51.245527Z","iopub.status.idle":"2023-10-18T06:25:51.662710Z","shell.execute_reply.started":"2023-10-18T06:25:51.245441Z","shell.execute_reply":"2023-10-18T06:25:51.661399Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"stock_id                         0\ndate_id                          0\nseconds_in_bucket                0\nimbalance_size                 220\nimbalance_buy_sell_flag          0\nreference_price                220\nmatched_size                   220\nfar_price                  2894342\nnear_price                 2857180\nbid_price                      220\nbid_size                         0\nask_price                      220\nask_size                         0\nwap                            220\ntarget                          88\ntime_id                          0\nrow_id                           0\ndtype: int64"},"metadata":{}}]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"For all the remaining NaN values, we will impute all the price columns as the average of the immediate previous and the immediate next values.","metadata":{}},{"cell_type":"code","source":"csv.sort_values(['stock_id', 'time_id'], inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-10-18T06:27:19.462857Z","iopub.execute_input":"2023-10-18T06:27:19.463199Z","iopub.status.idle":"2023-10-18T06:27:23.566623Z","shell.execute_reply.started":"2023-10-18T06:27:19.463175Z","shell.execute_reply":"2023-10-18T06:27:23.565806Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"1. All the 220 null values are for 4 specific stocks (131, 101, 19, 158) for one day each. To impute them, simple bfill or ffill would not work. Instead, we need to impute for time ti on the null-values day d with the corresponding value at ti on the day d - 1 and / or d + 1 (either bfill, ffill or average of the two), for each stock.\n\n2. Steps:","metadata":{}},{"cell_type":"code","source":"csv.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2023-10-18T06:27:33.208924Z","iopub.execute_input":"2023-10-18T06:27:33.209241Z","iopub.status.idle":"2023-10-18T06:27:33.806859Z","shell.execute_reply.started":"2023-10-18T06:27:33.209215Z","shell.execute_reply":"2023-10-18T06:27:33.805947Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"stock_id                         0\ndate_id                          0\nseconds_in_bucket                0\nimbalance_size                 220\nimbalance_buy_sell_flag          0\nreference_price                220\nmatched_size                   220\nfar_price                  2894342\nnear_price                 2857180\nbid_price                      220\nbid_size                         0\nask_price                      220\nask_size                         0\nwap                            220\ntarget                          88\ntime_id                          0\nrow_id                           0\ndtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"# Function to fill the missing data\n\ndef HandleMissingDayData(dataframe, fillType):\n    \"\"\"\n    Function to fill missing data with the previous and following day data at same timestamp.\n\n    Parameters:\n    fillType (boolean): -1 when just removing the rows\n                        0 when forward filling using previous day record\n                        1 when using mean of previous and following day record\n\n    \"\"\"\n    # imbalance_size, reference_price, matched_size,bid_price, ask_price, wap,\n    # far_price, near_price\n\n    nullDataMap = {}\n    nullDataFrame = dataframe[dataframe.reference_price.isnull()].copy()\n    stockIds = dataframe[dataframe.reference_price.isnull()].stock_id.unique()\n    # dateIds = dataframe[dataframe.reference_price.isnull()].date_id.unique()\n    for stock_id in stockIds:\n        # print(str(stock_id) + \": \")\n        # print(nullDataFrame[nullDataFrame['stock_id'] == stock_id].date_id.unique())\n        nullDataMap[stock_id] = nullDataFrame[nullDataFrame['stock_id'] == stock_id].date_id.unique()[0]\n\n    if (fillType == 0):\n        modifyingColumns = ['imbalance_size', 'matched_size', 'bid_price', 'ask_price', 'bid_size', 'ask_size', 'far_price', 'near_price', 'target', 'reference_price', 'wap']\n        for stock_id in nullDataMap.keys():\n\n            maskCurrent = (dataframe['stock_id'] == stock_id) & (dataframe['date_id'] == nullDataMap[stock_id])\n            maskPrevious = (dataframe['stock_id'] == stock_id) & (dataframe['date_id'] == nullDataMap[stock_id] - 1) # Doing this directly because there is no null value at date_id = 0\n            # maskNext = (dataframe['stock_id'] == stock_id) & (dataframe['date_id'] == nullDataMap[stock_id] + 1) # Doing this directly because there is no null value at date_id = 480\n            for timestamp in range(0, 541, 10):\n                for column in modifyingColumns:\n                #  'far_price', 'near_price' have NaN values upto 300\n                    dataframe.loc[maskCurrent & (dataframe['seconds_in_bucket'] == timestamp),\n                    column] = dataframe.loc[maskPrevious & (dataframe['seconds_in_bucket'] == timestamp), column].values[0]\n\n    elif (fillType == 1):\n        modifyingColumns = ['imbalance_size', 'matched_size', 'bid_price', 'ask_price', 'bid_size', 'ask_size', 'far_price', 'near_price', 'target', 'reference_price', 'wap']\n        for stock_id in nullDataMap.keys():\n            print(stock_id)\n            maskCurrent = (dataframe['stock_id'] == stock_id) & (dataframe['date_id'] == nullDataMap[stock_id])\n            maskPrevious = (dataframe['stock_id'] == stock_id) & (dataframe['date_id'] == nullDataMap[stock_id] - 1) # Doing this directly because there is no null value at date_id = 0\n            maskNext = (dataframe['stock_id'] == stock_id) & (dataframe['date_id'] == nullDataMap[stock_id] + 1) # Doing this directly because there is no null value at date_id = 480\n\n            for timestamp in range(0, 541, 10):\n                for column in modifyingColumns:\n                #  'far_price', 'near_price' have NaN values upto 300\n                    dataframe.loc[maskCurrent & (dataframe['seconds_in_bucket'] == timestamp),\n                    column] = np.mean([dataframe.loc[maskPrevious & (dataframe['seconds_in_bucket'] == timestamp), column].values[0],\n                                        dataframe.loc[maskNext & (dataframe['seconds_in_bucket'] == timestamp), column].values[0]])\n        if 'wap' in modifyingColumns:\n            dataframe['wap'] = dataframe.apply(lambda x: (x['ask_price'] * x['bid_size'] + x['bid_price'] * x['ask_size']) / (x['bid_size'] + x['ask_size']), axis=1)\n    return dataframe","metadata":{"execution":{"iopub.status.busy":"2023-10-18T06:28:05.693419Z","iopub.execute_input":"2023-10-18T06:28:05.693894Z","iopub.status.idle":"2023-10-18T06:28:05.708574Z","shell.execute_reply.started":"2023-10-18T06:28:05.693857Z","shell.execute_reply":"2023-10-18T06:28:05.707189Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"csv = HandleMissingDayData(csv, 1)","metadata":{"execution":{"iopub.status.busy":"2023-10-18T06:28:23.862501Z","iopub.execute_input":"2023-10-18T06:28:23.862876Z","iopub.status.idle":"2023-10-18T06:31:11.355217Z","shell.execute_reply.started":"2023-10-18T06:28:23.862848Z","shell.execute_reply":"2023-10-18T06:31:11.353947Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"19\n101\n131\n158\n","output_type":"stream"}]},{"cell_type":"code","source":"row_id_set = set(csv['row_id'])\nnon_values = []\nmissing_stock_set = set()\n\nfor i in range(0, 481):\n    for j in range(0, 55):\n        for k in range(0, 200):\n            value = f'{i}_{j*10}_{k}'\n            if value in row_id_set:\n                continue\n            else:\n                missing_stock_set.add(k)\n                non_values.append(value)\nprint(missing_stock_set)","metadata":{"execution":{"iopub.status.busy":"2023-10-18T06:33:12.892829Z","iopub.execute_input":"2023-10-18T06:33:12.893225Z","iopub.status.idle":"2023-10-18T06:33:19.858495Z","shell.execute_reply.started":"2023-10-18T06:33:12.893195Z","shell.execute_reply":"2023-10-18T06:33:19.857464Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"{99, 69, 102, 135, 199, 73, 78, 79, 150, 153, 156}\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Thus, rows with null values for the target variable have been removed.","metadata":{}},{"cell_type":"code","source":"missing_stock_set","metadata":{"execution":{"iopub.status.busy":"2023-10-18T06:33:19.859898Z","iopub.execute_input":"2023-10-18T06:33:19.860626Z","iopub.status.idle":"2023-10-18T06:33:19.866992Z","shell.execute_reply.started":"2023-10-18T06:33:19.860595Z","shell.execute_reply":"2023-10-18T06:33:19.865919Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"{69, 73, 78, 79, 99, 102, 135, 150, 153, 156, 199}"},"metadata":{}}]},{"cell_type":"code","source":"temp_df = temp_df[~temp_df['stock_id'].isin(missing_stock_set)]","metadata":{"execution":{"iopub.status.busy":"2023-10-18T06:34:44.964841Z","iopub.execute_input":"2023-10-18T06:34:44.965948Z","iopub.status.idle":"2023-10-18T06:34:46.072844Z","shell.execute_reply.started":"2023-10-18T06:34:44.965905Z","shell.execute_reply":"2023-10-18T06:34:46.070718Z"},"trusted":true},"execution_count":13,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m temp_df \u001b[38;5;241m=\u001b[39m \u001b[43mtemp_df\u001b[49m[\u001b[38;5;241m~\u001b[39mtemp_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstock_id\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39misin(missing_stock_set)]\n","\u001b[0;31mNameError\u001b[0m: name 'temp_df' is not defined"],"ename":"NameError","evalue":"name 'temp_df' is not defined","output_type":"error"}]},{"cell_type":"markdown","source":"#### Q2: Pairwise Corralation Table and Explanation. 10 pts","metadata":{}},{"cell_type":"code","source":"correlation_matrix = csv.corr(method='pearson')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(10, 8))\nsns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=.5)\nplt.title('Pearson Correlation Heatmap')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Reasons for high correlation values:\n1. **reference_price** and **bid_price**: a high positive correlation exists between these two columns. It is positive because the bid price for a stock can only be â‰¥ its reference price. Further, a high absolute value of the correlation might suggest that a bidder is not likely to go much higher than what most people believe is a fair prrice for the stock (which is represented by the reference_price).\n2. **wap** and **ask_price**, **wap** and **bid_price**: weighted average price is simply a linear combination of **ask_price** and **bid_price**, given by:\n>\n> `wap = (bid_price * ask_size + bid_size * ask_price) / (bid_size + ask_size)`\n>\nAs a result of this linear combination, the value of \"wap\" exhibits a lot of correlation with bid_price and ask_price. Note that if we were using a regression model, it would probably not be wise to include all three of these features at the same time, as regression models require as low multicollinearity between different independent variables as possible so as to obtain a good result, and a high correlation often indicates high multicollinearity.","metadata":{}},{"cell_type":"markdown","source":"#### Q3: Average records stockID vs Day, 25 pts\ndistance function between entries\n- a. autocorrelation, 10 pts\n- b. measure the distance, 5 pts\n- c. clustering algorithm, 10 pts","metadata":{}},{"cell_type":"code","source":"csv.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"csv.head(1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"average_records = csv.groupby(['stock_id', 'date_id']).agg({\n    'seconds_in_bucket': 'mean',\n    'imbalance_size': 'mean',\n    'imbalance_buy_sell_flag': 'mean',\n    'reference_price': 'mean',\n    'matched_size': 'mean',\n    'bid_price': 'mean',\n    'bid_size': 'mean',\n    'ask_price': 'mean',\n    'ask_size': 'mean',\n    'wap': 'mean',\n    'target': 'mean',\n    'time_id': 'count'  # You can change this depending on your needs\n}).reset_index()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"average_records.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"average_records.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import statsmodels.api as sm\nimport statsmodels.formula.api as smf\n# Sort the data by 'stock_id', 'date_id', and 'time_id' in ascending order\ncsv.sort_values(by=['stock_id', 'date_id', 'time_id'], inplace=True)\n\n# Group data by 'stock_id' and 'date_id' and select the last record for each group\n# last_records = csv.groupby(['stock_id', 'date_id']).last().reset_index()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"autocorrelation_results = {}\n\nunique_stock_ids = csv['stock_id'].unique()\n\nfor stock_id in unique_stock_ids:\n    autocorrelation_results[stock_id] = [[], []]\n#     stock_data = last_records[last_records['stock_id'] == stock_id]\n#     stock_data = last_records[last_records['stock_id'] == stock_id].copy()\n    stock_data = average_records[average_records['stock_id'] == stock_id].copy()\n#     max_autocorr = -1\n#     optimal_lag = 0\n    \n    for shift in range(-10, 11):\n        if shift != 0:\n            temp = stock_data['target'].autocorr(lag=shift)\n            autocorrelation_results[stock_id][0].append(shift)\n            autocorrelation_results[stock_id][1].append(temp)\n#         if temp > max_autocorr:\n#             max_autocorr = temp\n#             optimal_lag = shift\n            \n    \n    # Calculate the average distance for each day\n#     average_distance = stock_data.groupby('date_id')['distance'].mean()\n\n    # Calculate autocorrelation for lags from -10 to 10\n#     autocorrelation_results[stock_id] = []\n\n#     for lag in range(-10, 11):\n#         autocorr = average_distance.autocorr(lag=lag)\n#         autocorrelation_results[stock_id].append((lag, autocorr))\n\n# Perform a hypothesis test for each stock to determine statistical significance\n# sig = []\n# non_sig = []\n# for stock_id, results in autocorrelation_results.items():\n#     autocorrelation_df = pd.DataFrame(results, columns=['Lag', 'Autocorrelation'])\n#     result = smf.ols('Autocorrelation ~ Lag', data=autocorrelation_df).fit()\n\n#     # Check if the autocorrelation is statistically significant\n#     p_value = result.pvalues['Lag']\n# #     print(f\"p value is {p_value}\")\n#     alpha = 0.5  # Significance level\n\n#     if abs(p_value) < abs(alpha):\n#         sig.append(stock_id)\n# #         print(f\"Stock {stock_id}: The autocorrelation is statistically significant (p-value < 0.05).\")\n#     else:\n#         non_sig.append(stock_id)\n#         print(f\"Stock {stock_id}: The autocorrelation is not statistically significant (p-value >= 0.05).\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"csv = csv.sort_values(by=['stock_id', 'date_id', 'time_id'])\n\n# Define the number of days to look back and ahead (10 in this case).\nlook_back_days = 10\nlook_ahead_days = 10\n\n# Create an empty DataFrame to store the autocorrelation results.\nautocorrelation_results = pd.DataFrame(columns=['stock_id', 'autocorrelation'])\n\n# Group the data by 'stock_id'.\ngrouped = df.groupby('stock_id')\n\n# Iterate through each group (stock_id).\nfor stock_id, group in grouped:\n    # Calculate autocorrelation for each 'target' value in the group.\n    autocorrelations = []\n    for i in range(-look_back_days, look_ahead_days + 1):\n        autocorrelation = group['target'].autocorr(lag=i)\n        autocorrelations.append(autocorrelation)\n\n    # Create a DataFrame for the results and append it to the main results DataFrame.\n    stock_autocorr_df = pd.DataFrame({'stock_id': [stock_id] * (2 * look_back_days + 1), 'autocorrelation': autocorrelations})\n    autocorrelation_results = autocorrelation_results.append(stock_autocorr_df, ignore_index=True)\n\n# Print or save the autocorrelation results.\nprint(autocorrelation_results)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(autocorrelation_results.keys())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(sig), len(non_sig))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(csv['stock_id'].unique())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"csv.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"csv_stockwise = {}\nunique_stocks = csv['stock_id'].unique()\nfor stock in unique_stocks:\n    csv_stockwise[stock] = csv[csv['stock_id'] == stock]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(11,4), dpi= 80)\npd.plotting.autocorrelation_plot(csv_stockwise[0].loc[:, 'bid_price'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Q4: Closing trajectory of stocks on each day highly correlated, 25 pts\n- a. Make three plots, 10 pts\n- b. permutation test for statistical confidence, p-value, 15 pts","metadata":{}},{"cell_type":"code","source":"csv.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"temp_df = csv.groupby(['stock_id', 'date_id']).agg({\n    'seconds_in_bucket': 'mean',\n    'imbalance_size': 'mean',\n    'imbalance_buy_sell_flag': 'mean',\n    'reference_price': 'prod',\n    'matched_size': 'mean',\n    'bid_price': 'prod',\n    'bid_size': 'mean',\n    'ask_price': 'prod',\n    'ask_size': 'mean',\n    'wap': 'mean',\n    'target': 'mean',\n    'time_id': 'count'  # You can change this depending on your needs\n}).reset_index()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"temp_df.head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"perf_df = pd.DataFrame()\ntemp_df[\"row_number\"] = temp_df.groupby(\"date_id\").cumcount() + 1\nperf_df = temp_df.pivot(index='date_id', columns='row_number', values=['reference_price'])\nperf_df.columns = [f'stock_{stock_id}' for stock_id in temp_df['stock_id'].unique()]\nperf_df.reset_index(inplace=True)\n# perf_df['stock_134'].head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"perf_df.head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for col in perf_df.columns:\n    if col != 'date_id':\n        perf_df[col] = perf_df[col].apply(lambda x : 1 if x > 1 else -1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"perf_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"columns_to_be_added = [col for col in perf_df.columns]\ncolumns_to_be_added.remove('date_id')\nperf_df['total']= perf_df[columns_to_be_added].sum(axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"perf_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import scipy.stats as stats\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=RuntimeWarning, message=\"Precision loss occurred in moment calculation due to catastrophic cancellation.\")\nup_days = []\ndown_days = []\nalpha = 0.05\nfor date in perf_df['date_id'].unique():\n    dist = perf_df[perf_df['date_id'] == date][columns_to_be_added].values.flatten().tolist()\n    t_statistic, p_value = stats.ttest_1samp(dist, 0)\n    if p_value < alpha / 2:\n        if t_statistic >= 0:\n            up_days.append(date)\n        else:\n            down_days.append(date)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"{len(up_days)} up days\")\nprint(f\"{len(down_days)} down days\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"temp_df[temp_df['stock_id'] == 198].head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Three plots","metadata":{}},{"cell_type":"markdown","source":"### 1. Net stock performance vs. frequency","metadata":{}},{"cell_type":"code","source":"plt.hist(perf_df['total'])\nplt.xlabel('Net stock performance')\nplt.ylabel('Frequency')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This graph describes the net performance of all stocks for all days vs. frequency. The graph indicates a large volume of days experiencing stocks performing below average in a statistically significant manner. This is backed by our values of number of stocks performing well and number of stocks not performing well, as computed previously.","metadata":{}},{"cell_type":"markdown","source":"### 2. Overall market performance during first 100 days to identify the bad days.","metadata":{}},{"cell_type":"code","source":"# stock_ids = temp_df['stock_id'].unique()[0 : 50]\ndays = temp_df['date_id'].unique()[0:100]\ny = perf_df['total'][0:100]\nplt.figure(figsize=(12, plt.gcf().get_figheight()))\nplt.plot(days, y, linestyle='-')\n# plt.axhline(y=0, color='r', linestyle='-')\nrect_start_x = 0\nrect_end_x = 100\nrect_bottom = -10\nrect_top = 10\nplt.fill_between([rect_start_x, rect_end_x], rect_bottom, rect_top, color='gray', alpha=1, label='Rectangle Area')\nplt.xlabel('Days')\nplt.ylabel('Stock performance')\nplt.title('How the market performs during the first 100 days')\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For the first 100 days, the days when the stock market performs the worst (i.e deep troughs) occur at approximately regular intervals of 6 to 8 days, except for when no deep trough occurs between 60 and 80. Note that these deep troughs are only the worst performances for the stock market. There are of course other days between each successive deep troughs where the stock market performs bad in a statistically significant manner, but maybe not to the extent of these regularly occurring deep troughs. Although the visual confirmation may be misleading, one can see that overall in the graph, there are more bad days than good. This sample of the first 100 days displays similar results as the entire population (i.e there are more bad days than good). To approximate statistical significance, I have considered a window of \"total\" in the range [-10, 10] as being just good or bad 'by chance'.","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"#### Q5: Best prediction model, any approaches, 25 pts","metadata":{}},{"cell_type":"code","source":"import xgboost as xgb\nfrom sklearn.model_selection import train_test_split\n\nX = temp_df[]\nX, y = iris.data, iris.target\n\n# Split the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Create and configure the XGBoost classifier\nmodel = xgb.XGBClassifier(objective=\"multi:softprob\", random_state=42)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"temp_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Q6: submit model on Kaggle, 0 pts\nPublic Score: \\\nPrivate Score: \\\nKaggle profile link: \\\nScreenshot(s):\n","metadata":{}}]}